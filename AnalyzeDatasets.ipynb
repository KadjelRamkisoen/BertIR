{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Notebook to check the duplicates within a dataset and select the one for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "\n",
    "# File names\n",
    "dev_data = 'dev_data.csv'\n",
    "all_docs = 'all_docs.csv'\n",
    "dev_queries = 'dev_queries.csv'\n",
    "test_data = 'test_data.csv'\n",
    "training_data = 'training_data.csv'\n",
    "lucene_data = 'raw_dev_Lucene_retrievals.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all data\n",
    "df_dev_data = pd.read_csv(dev_data)\n",
    "df_all_docs = pd.read_csv(all_docs)\n",
    "df_dev_queries = pd.read_csv(dev_queries)\n",
    "df_test_data = pd.read_csv(test_data)\n",
    "df_training_data = pd.read_csv(training_data)\n",
    "df_lucene_data = pd.read_csv(lucene_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates in al data\n",
    "dup_dev_data = df_dev_data.duplicated(keep='first')\n",
    "dup_all_docs = df_all_docs.duplicated(keep='first')\n",
    "dup_dev_queries = df_dev_queries.duplicated(keep='first')\n",
    "dup_test_data = df_test_data.duplicated(keep='first')\n",
    "dup_training_data = df_training_data.duplicated(keep='first')\n",
    "dup_lucene_data = df_lucene_data.duplicated(keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add duplicate column to the file\n",
    "df_dev_data['duplicate'] = dup_dev_data\n",
    "df_all_docs['duplicate'] = dup_all_docs\n",
    "df_dev_queries['duplicate'] = dev_queries\n",
    "df_test_data['duplicate'] = test_data\n",
    "df_training_data['duplicate'] = dup_training_data\n",
    "df_lucene_data['duplicate'] = dup_lucene_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{dev_data} duplicates\")\n",
    "df_dev_data[df_dev_data['duplicate']== True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{all_docs} duplicates\")\n",
    "df_all_docs[df_all_docs['duplicate']== True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{dev_queries} duplicates\")\n",
    "df_dev_queries[df_dev_queries['duplicate']== True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{test_data} duplicates\")\n",
    "df_test_data[df_test_data['duplicate']== True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{training_data} duplicates\")\n",
    "df_training_data[df_training_data['duplicate']== True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{lucene_data} duplicates\")\n",
    "df_lucene_data[df_lucene_data['duplicate']== True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_docs.drop_duplicates(subset=['doc_number', 'doc_text'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of documents\n",
    "print(f'df_dev_data: {df_dev_data.shape}')\n",
    "print(f'df_all_docs: {df_all_docs.shape}')\n",
    "print(f'df_dev_queries: {df_dev_queries.shape}')\n",
    "print(f'df_test_data: {df_test_data.shape}')\n",
    "print(f'df_training_data: {df_training_data.shape}')\n",
    "print(f'df_lucene_data: {df_lucene_data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check column names for merging dataframes\n",
    "print(df_dev_data.columns)\n",
    "print(df_all_docs.columns)\n",
    "print(df_dev_queries.columns)\n",
    "print(df_test_data.columns)\n",
    "print(df_training_data.columns)\n",
    "print(df_lucene_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check duplicated between the different files\n",
    "# all_docs\n",
    "df_1 = df_all_docs[['doc_number', 'doc_text']].append(df_dev_data[['doc_number', 'doc_text']].drop_duplicates())\n",
    "df_2 = df_all_docs[['doc_number', 'doc_text']].append(df_test_data[['doc_number', 'doc_text']].drop_duplicates())\n",
    "df_3 = df_all_docs[['doc_number', 'doc_text']].append(df_training_data[['doc_number', 'doc_text']].drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check duplicated between the different files\n",
    "# dev_data\n",
    "df_4 = df_dev_data[['Query_number', 'doc_number', 'Query', 'doc_text']].append(df_test_data[['Query_number', 'doc_number', 'Query', 'doc_text']].drop_duplicates())\n",
    "df_5 = df_dev_data[['Query_number', 'doc_number', 'Query', 'doc_text']].append(df_training_data[['Query_number', 'doc_number', 'Query', 'doc_text']].drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check duplicated between the different files\n",
    "# test_data\n",
    "df_6 = df_test_data[['Query_number', 'doc_number', 'Query', 'doc_text']].append(df_training_data[['Query_number', 'doc_number', 'Query', 'doc_text']].drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find duplicates\n",
    "dup_df_1 = df_1.duplicated(keep=False)\n",
    "df_1['duplicate'] = dup_df_1\n",
    "dup_df_2 = df_2.duplicated(keep=False)\n",
    "df_2['duplicate'] = dup_df_2\n",
    "dup_df_3 = df_3.duplicated(keep=False)\n",
    "df_3['duplicate'] = dup_df_3\n",
    "dup_df_4 = df_4.duplicated(keep=False)\n",
    "df_4['duplicate'] = dup_df_4\n",
    "dup_df_5 = df_5.duplicated(keep=False)\n",
    "df_5['duplicate'] = dup_df_5\n",
    "dup_df_6 = df_6.duplicated(keep=False)\n",
    "df_6['duplicate'] = dup_df_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1[df_1['duplicate']==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2[df_2['duplicate']==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3[df_3['duplicate']==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_4[df_4['duplicate']==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5[df_5['duplicate']==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_6[df_6['duplicate']==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_training_data = pd.read_csv(training_data)\n",
    "df_training_data.sample(frac = 1)\n",
    "train = df_training_data[:70000]\n",
    "test = df_training_data[70000:75000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open ('train2.pkl', 'wb') as f:\n",
    "    pickle.dump(train, f)\n",
    "    \n",
    "with open ('test2.pkl', 'wb') as g:\n",
    "    pickle.dump(test, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('train.csv', sep='\\t')\n",
    "test.to_csv('test.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
