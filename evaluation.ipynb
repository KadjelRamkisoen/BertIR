{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A notebook to evaluate our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data = '../dev_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "\n",
    "model_checkpoint = \"cross-encoder/stsb-TinyBERT-L-4\"\n",
    "model = CrossEncoder(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base evaluation (only pretrained model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_base = model.predict(df[['Query', 'doc_text']].values.tolist(), show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"score_base\"] = scores_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print(\"results on base pretrained model: \" , metrics.roc_auc_score(y_true=df['label'], y_score=df[\"score_base\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_tuned = model.predict(df[['Query', 'doc_text']].values.tolist(), show_progress_bar=True)\n",
    "df[\"score_tuned\"] = scores_tuned\n",
    "\n",
    "print(\"results on base pretrained model + tune: \" , \n",
    "      metrics.roc_auc_score(y_true=df['label'], y_score=df[\"score_tuned\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the long document texts overlapping = 20 \n",
    "def get_split480(text1):\n",
    "    l_total = []\n",
    "    l_parcial = []\n",
    "    if len(text1.split())// 479>0:\n",
    "        n = len(text1.split())//479\n",
    "    else: \n",
    "        n = 1\n",
    "    for w in range(n):\n",
    "        if w == 0:\n",
    "            l_parcial = text1.split()[:479]\n",
    "            l_total.append(\" \".join(l_parcial))\n",
    "        else:\n",
    "            l_parcial = text1.split()[w*459:w*459 + 479]\n",
    "            l_total.append(\" \".join(l_parcial))\n",
    "    return l_total\n",
    "\n",
    "# Split the document text\n",
    "df['text_split1'] = df['doc_text'].apply(get_split480)\n",
    "#df_all_docs.head()\n",
    "\n",
    "docs_l = []\n",
    "label_l = []\n",
    "index_l =[]\n",
    "query_l = []\n",
    "query_n = []\n",
    "doc_n = []\n",
    "score_b = []\n",
    "score_t = []\n",
    "for idx,row in df.iterrows():\n",
    "    for l in row['text_split1']:\n",
    "        docs_l.append(l)\n",
    "        label_l.append(row['label'])\n",
    "        query_l.append(row['Query'])\n",
    "        doc_n.append(row['doc_number'])\n",
    "        query_n.append(row['Query_number'])\n",
    "        \n",
    "        index_l.append(idx)\n",
    "len(docs_l), len(label_l), len(index_l)\n",
    "\n",
    "# Create a new dataframe with the splitted documents\n",
    "df_chunked = pd.DataFrame({\"doc_text\":docs_l, 'label':label_l, 'Query': query_l, \n",
    "                       \"doc_number\": doc_n, \"query_number\": query_n})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chunked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../stsb-TinyBERT-L-4-finetuned_auc_151221-5-001\"\n",
    "model =  CrossEncoder(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_tuned = model.predict(df_chunked[['Query', 'doc_text']].values.tolist(), show_progress_bar=True)\n",
    "df_chunked[\"score_tuned\"] = scores_tuned\n",
    "\n",
    "print(\"results on base pretrained model + tune: \" , \n",
    "      metrics.roc_auc_score(y_true=df_chunked['label'], y_score=df_chunked[\"score_tuned\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"stsb-TinyBERT-L-4-finetuned_auc_151221-top3\"\n",
    "model3 =  CrossEncoder(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_top3 = model3.predict(df_chunked[['Query', 'doc_text']].values.tolist(), show_progress_bar=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chunked[\"score_top3\"] = scores_top3\n",
    "\n",
    "print(\"results on top 3 model: \" , \n",
    "      metrics.roc_auc_score(y_true=df_chunked['label'], y_score=df_chunked[\"score_top3\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"stsb-TinyBERT-top1\"\n",
    "model1 =  CrossEncoder(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_top1 = model1.predict(df_chunked[['Query', 'doc_text']].values.tolist(), show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chunked[\"score_top1\"] = scores_top1\n",
    "\n",
    "print(\"results on top 1 model: \" , \n",
    "      metrics.roc_auc_score(y_true=df_chunked['label'], y_score=df_chunked[\"score_top1\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chunked.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chunked[(df_chunked['Query_number'] == 1089071) & (df_chunked['doc_number'] == 29215)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = pd.DataFrame(df_chunked.groupby(by=[\"query_number\", \"doc_number\", \"label\"]).max([\"score_top3\", \"score_top1\", \"score_tuned\"])).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"results on top 1 model: \" , \n",
    "      metrics.roc_auc_score(y_true=grouped['label'], y_score=grouped[\"score_top1\"]))\n",
    "print(\"results on top 3 model: \" , \n",
    "      metrics.roc_auc_score(y_true=grouped['label'], y_score=grouped[\"score_top3\"]))\n",
    "print(\"results on chunked: \" , \n",
    "      metrics.roc_auc_score(y_true=grouped['label'], y_score=grouped[\"score_tuned\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"results on top 1 model: \" , \n",
    "      metrics.roc_auc_score(y_true=df['label'], y_score=df[\"score_top1\"]))\n",
    "print(\"results on top 3 model: \" , \n",
    "      metrics.roc_auc_score(y_true=df['label'], y_score=df[\"score_top3\"]))\n",
    "print(\"results on chunked: \" , \n",
    "      metrics.roc_auc_score(y_true=df['label'], y_score=df[\"score_tuned\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"results on top 1 model: \" , \n",
    "      metrics.roc_auc_score(y_true=df_chunked['label'], y_score=df_chunked[\"score_top1\"]))\n",
    "print(\"results on top 3 model: \" , \n",
    "      metrics.roc_auc_score(y_true=df_chunked['label'], y_score=df_chunked[\"score_top3\"]))\n",
    "print(\"results on chunked: \" , \n",
    "      metrics.roc_auc_score(y_true=df_chunked['label'], y_score=df_chunked[\"score_tuned\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped[grouped['label'] == 1]['score_top3'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped[grouped['label'] == 0]['score_top3'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"../stsb-TinyBERT-L-4-finetuned_auc_151221-top3\"\n",
    "model = CrossEncoder(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.predict(df_chunked[['Query', 'doc_text']].values.tolist(), show_progress_bar=True)\n",
    "df_chunked[\"score\"] = scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print(\"results withour aggregation: \" , \n",
    "      metrics.roc_auc_score(y_true=df_chunked['label'], y_score=df_chunked[\"score\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chunked.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = pd.DataFrame(df_chunked.groupby(by=[\"query_number\", \"doc_number\", \"label\"])[\"score\"].max()).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.score.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.score.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "grouped['pred_label'] = np.where(grouped['score'] > 0.64, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped['pred_label'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = grouped.label\n",
    "prediction = grouped['pred_label'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report, accuracy_score, f1_score\n",
    "\n",
    "print('Accuracy:', accuracy_score(y_test, prediction))\n",
    "print('F1 score:', f1_score(y_test, prediction))\n",
    "print('Recall:', recall_score(y_test, prediction))\n",
    "print('Precision:', precision_score(y_test, prediction))\n",
    "print('\\n clasification report:\\n', classification_report(y_test,prediction))\n",
    "print('\\n confussion matrix:\\n',confusion_matrix(y_test, prediction))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = np.linspace(0, 1, 21)\n",
    "\n",
    "data_ones = grouped[grouped['label'] == 1]['score']\n",
    "digitized_ones = np.digitize(data_ones, base)\n",
    "values_ones = [data_ones[digitized_ones == i].count() for i in range(1, len(base))]\n",
    "\n",
    "data_all = grouped['score']\n",
    "digitized_all = np.digitize(data_all, base)\n",
    "values_all = [data_all[digitized_all == i].count() for i in range(1, len(base))]\n",
    "\n",
    "cumulative_all = np.cumsum(values_all)\n",
    "cumulative_ones = np.cumsum(values_ones)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = np.linspace(0, 1, 51)\n",
    "\n",
    "data_ones = fig, ax1 = plt.subplots(figsize=(10,5))\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Score')\n",
    "ax1.set_ylabel('Number of records')\n",
    "\n",
    "p1 = ax1.hist([grouped[grouped['label'] == 1]['score'], grouped[grouped['label'] == 0]['score']], bins=50, alpha=0.5, range=(0,1), label=['1', '0'])\n",
    "ax1.tick_params(axis='y')\n",
    "# ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "# ax2.set_ylabel('%')  # we already handled the x-label with ax1\n",
    "# p2 = ax2.p.lot(base[:-1]+0.025, cumulative_ones / cumulative_ones[-1], c='blue',marker=\"o\", alpha=0.5, label = 'cumm for ones')\n",
    "# p3 = ax2.plot(base[:-1]+0.025, (len(grouped['score'])-cumulative_all) / cumulative_all[-1] , c='green',marker=\"o\", alpha=0.5, label = 'cumm for all')\n",
    "\n",
    "# for container in ax1.containers:\n",
    "#     ax1.bar_label(container)\n",
    "# x0 = base[:-1]+0.025.\n",
    "# y1 = cumulative_ones / cumulative_ones[-1]\n",
    "# y2 = (len(grouped['score'])-cumulative_all) / cumulative_all[-1]\n",
    "\n",
    "# for x, y, text in zip(x0, y1, y1):\n",
    "#     text = str(round(text * 100)) + \"%\"\n",
    "#     plt.text(x, y, text)\n",
    "# for x, y, text in zip(x0, y2, y2):\n",
    "#     text = str(round(text * 100)) + \"%\"\n",
    "#     plt.text(x, y, text)\n",
    "fig.legend(loc=\"upper right\")\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.show()\n",
    "# digitized_ones = numpy.digitize(data_ones, base)\n",
    "# values_ones = [data_ones[digitized_ones == i].count() for i in range(1, len(base))]\n",
    "\n",
    "# data_all = grouped['score']\n",
    "# digitized_all = numpy.digitize(data_all, base)\n",
    "# values_all = [data_all[digitized_all == i].count() for i in range(1, len(base))]\n",
    "\n",
    "# cumulative_all = np.cumsum(values_all)\n",
    "# cumulative_ones = np.cumsum(values_ones)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"../test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Split the document text\n",
    "test_df['text_split1'] = test_df['doc_text'].apply(get_split480)\n",
    "#df_all_docs.head()\n",
    "\n",
    "docs_l = []\n",
    "index_l =[]\n",
    "query_l = []\n",
    "query_n = []\n",
    "doc_n = []\n",
    "for idx,row in test_df.iterrows():\n",
    "    for l in row['text_split1']:\n",
    "        docs_l.append(l)\n",
    "        query_l.append(row['Query'])\n",
    "        doc_n.append(row['doc_number'])\n",
    "        query_n.append(row['Query_number'])\n",
    "        \n",
    "        index_l.append(idx)\n",
    "len(docs_l), len(label_l), len(index_l)\n",
    "\n",
    "# Create a new dataframe with the splitted documents\n",
    "test_df_chunked = pd.DataFrame({\"doc_text\":docs_l,\n",
    "                                'Query': query_l, \n",
    "                       \"doc_number\": doc_n, \"query_number\": query_n})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_chunked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_test = model.predict(test_df_chunked[['Query', 'doc_text']].values.tolist(), show_progress_bar=True)\n",
    "test_df_chunked[\"score\"] = scores_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_chunked.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_test = pd.DataFrame(test_df_chunked.groupby(by=[\"query_number\", \"doc_number\"])[\"score\"].max()).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_test['pred_label'] = np.where(grouped_test['score'] > 0.64, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_test.sort_values(['query_number', \"score\"], ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_test.to_csv('neural model rank.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check another model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"../stsb-TinyBERT-top3-70000/content/stsb-TinyBERT-L-4-finetuned_auc_161221-top3\"\n",
    "model1 = CrossEncoder(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores1 = model1.predict(df_chunked[['Query', 'doc_text']].values.tolist(), show_progress_bar=True)\n",
    "df_chunked[\"score1\"] = scores1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print(\"results withour aggregation: \" , \n",
    "      metrics.roc_auc_score(y_true=df_chunked['label'], y_score=df_chunked[\"score1\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chunked.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = pd.DataFrame(df_chunked.groupby(by=[\"query_number\", \"doc_number\", \"label\"])[\"score\", \"score1\"].max()).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"prev: \", grouped.score.mean())\n",
    "print(\"new: \", grouped.score1.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"prev: \", grouped.score.max())\n",
    "print(\"new: \", grouped.score1.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"prev: \", grouped.score.min())\n",
    "print(\"new: \", grouped.score1.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.roc_auc_score(y_true=grouped['label'], y_score=grouped[\"score1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.roc_auc_score(y_true=grouped['label'], y_score=grouped[\"score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "grouped['pred_label1'] = np.where(grouped['score1'] > 0.63, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped['pred_label1'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = grouped.label\n",
    "prediction1 = grouped['pred_label1'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report, accuracy_score, f1_score\n",
    "\n",
    "print('Accuracy:', accuracy_score(y_test, prediction1))\n",
    "print('F1 score:', f1_score(y_test, prediction1))\n",
    "print('Recall:', recall_score(y_test, prediction1))\n",
    "print('Precision:', precision_score(y_test, prediction1))\n",
    "print('\\n clasification report:\\n', classification_report(y_test,prediction1))\n",
    "print('\\n confussion matrix:\\n',confusion_matrix(y_test, prediction1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = np.linspace(0, 1, 21)\n",
    "\n",
    "data_ones = grouped[grouped['label'] == 1]['score1']\n",
    "digitized_ones = np.digitize(data_ones, base)\n",
    "values_ones = [data_ones[digitized_ones == i].count() for i in range(1, len(base))]\n",
    "\n",
    "data_all = grouped['score1']\n",
    "digitized_all = np.digitize(data_all, base)\n",
    "values_all = [data_all[digitized_all == i].count() for i in range(1, len(base))]\n",
    "\n",
    "cumulative_all = np.cumsum(values_all)\n",
    "cumulative_ones = np.cumsum(values_ones)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = np.linspace(0, 1, 21)\n",
    "\n",
    "data_ones = fig, ax1 = plt.subplots(figsize=(15,8))\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Score')\n",
    "ax1.set_ylabel('Number of values')\n",
    "\n",
    "p1 = ax1.hist([grouped[grouped['label'] == 1]['score1'], grouped[grouped['label'] == 0]['score1']], bins=20, alpha=0.5, range=(0,1), label=['1', '0'])\n",
    "ax1.tick_params(axis='y')\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('%')  # we already handled the x-label with ax1\n",
    "p2 = ax2.plot(base[:-1]+0.025, cumulative_ones / cumulative_ones[-1], c='blue',marker=\"o\", alpha=0.5, label = 'cumm for ones')\n",
    "p3 = ax2.plot(base[:-1]+0.025, (len(grouped['score1'])-cumulative_all) / cumulative_all[-1] , c='green',marker=\"o\", alpha=0.5, label = 'cumm for all')\n",
    "\n",
    "# for container in ax1.containers:\n",
    "#     ax1.bar_label(container)\n",
    "x0 = base[:-1]+0.025\n",
    "y1 = cumulative_ones / cumulative_ones[-1]\n",
    "y2 = (len(grouped['score1'])-cumulative_all) / cumulative_all[-1]\n",
    "\n",
    "for x, y, text in zip(x0, y1, y1):\n",
    "    text = str(round(text * 100)) + \"%\"\n",
    "    plt.text(x, y, text)\n",
    "for x, y, text in zip(x0, y2, y2):\n",
    "    text = str(round(text * 100)) + \"%\"\n",
    "    plt.text(x, y, text)\n",
    "fig.legend(loc=\"upper right\")\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.show()[grouped['label'] == 1]['score1']\n",
    "digitized_ones = numpy.digitize(data_ones, base)\n",
    "values_ones = [data_ones[digitized_ones == i].count() for i in range(1, len(base))]\n",
    "\n",
    "data_all = grouped['score1']\n",
    "digitized_all = numpy.digitize(data_all, base)\n",
    "values_all = [data_all[digitized_all == i].count() for i in range(1, len(base))]\n",
    "\n",
    "cumulative_all = np.cumsum(values_all)\n",
    "cumulative_ones = np.cumsum(values_ones)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
